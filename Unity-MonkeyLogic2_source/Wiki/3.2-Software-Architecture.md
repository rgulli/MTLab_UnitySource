# Overview
This section describes in details the different classes used to create the full experimental system. In a nutshell, Unity is controlling the entire task and trials progression, sending its frame and trial data over LSL streams. MonkeyLogic listens to these streams, saves the data to disk and terminates trials properly (e.g. juice reward and words to Cerebus). MonkeyLogic has some very limited control over Unity, namely Start, Stop, Pause, Resume and sending eye calibration data. This control is provided over another LSL stream from MonkeyLogic to Unity. 

The most complex part of the system is the Unity framework, but it should require very little modification to generate new tasks. The following figure presents a detailed overview of the different components. 

![Software](https://github.com/Doug1983/MTLab_UnitySource/blob/master/Documentation/img/Unity_ML_Doc.jpg)

## MonkeyLogic ML_UnityVR_Task
Since the [[MonkeyLogic Computer|3.1-Hardware-Architecture#monkeylogic-computer]] is mostly a passive observer of what's happening in Unity, it only needs a single task to record all possible Unity Experiments. 

The task architecture is fairly straightforward: 
* The _UnityVR.txt_ conditions file points to a custom userloop function file.
* The _UnityVR_userloop.m_ file defines the timing file to use.
* The **UnityVR.m** timing file contains a single scene with a custom tracker/adapter pair, waiting for predefined trial outcomes to trigger trial end, reward delivery and data saving to disk. 
* The **lslAdapter.m** script gets data from the **lslTracker.m** and checks whether the predefined outcomes are met to terminate the single scene (i.e. entire trial).
* The **lslTracker** simply acquires the available data from the two LSL streams coming from Unity (see next section).
* The **alert_function.m** file handles the creation of the two LSL inlets and the control outlet. The control outlet sends eye calibration data and can be used to start/stop or pause/resume the Unity task. 

## LSL Streams
The following table details the parameters of the three LSL streams used. 

|      Direction      	|          Name          	|    Content   Type    	|    Number of Channels	|    Rate    	|    Channel   Format    	|    Unique   ID    	|
|:-------------------:	|:----------------------:	|:--------------------:	|:-----------------:	|:----------:	|------------------------	|-------------------	|
|    ML ->   Unity    	|    ML_ControlStream    	|        Markers       	|         1         	|      0 (irregular)     	|    cf_string           	|    control1214    	|
|    Unity   -> ML    	|      ML_FrameData      	|         Unity        	|         22        	|      0 (irregular)     	|    cf_double64         	|    frame1214      	|
|    Unity   -> ML    	|    ML_TrialData        	|    Markers           	|    1              	|    0 (irregular)       	|    cf_string           	|    trial1214      	|

#### ML_ControlStream
As stated above, it sends basic start/stop commands and eye calibration data to Unity so it can get the raw eye signal from the eye tracker and compute foveation targets on the fly. 

#### ML_FrameData
Unity publishes data to this stream on every frame to enable complete trial reconstruction. Since all values sent from Unity are numbers, we can define it with 22 “channels”, to which 4 are added in the MonkeyLogic **lslTracker.m**: 
1.	Player position X: in Unity units
2.	Player position Y: same as X
3.	Player position Z: same as X
4.	Player rotation: in degrees
5.	Joystick position X: in relative value [0-1], also accepts keyboard, mouse and gamepad input
6.	Joystick position Y: same as X
7.	Player collision state: instance ID of object colliding with player, such as triggers or targets
8.	Gaze position X: position in degrees
9.	Gaze position Y: same as X
10.	Gaze target ID #1: instance ID of foveated object
11.	Gaze target ID #2
12.	Gaze target ID #3
13.	Gaze target ID #4
14.	Gaze target ID #5
15.	Gaze ray counts #1: number of rays hitting gaze target #1 out of 33
16.	Gaze ray counts #2
17.	Gaze ray counts #3
18.	Gaze ray counts #4
19.	Gaze ray counts #5
20.	Trial state index: integer value of trial state
21.	Photodiode intensity: grayscale value of flashing square to synchronize monitor with photodiode
22.	Unity computer LSL time

Added by **lslTracker.m**:

23.	LSL time correction between Unity (remote) and ML (local) clocks: remote clock = local clock + time correction
24.	Sample time: timestamp in Unity (remote) clock of sample being pushed to stream
25.	ML local LSL time: timestamp when sample is obtained
26.	ML trial time: time in milliseconds of ML trial (all other ML events use this time scale)

These values are stored during a trial for each frame and saved to the `UserVars.VR_Data` structure in MonkeyLogic _.bhv2_ files.

#### ML_TrialData
Unity publishes data to this stream and the end of every trial. Since the values have different formats (e.g. numbers, vectors, strings,...) we convert them all to a JSON formatted string. The **lslAdapter.m** script waits to receive data from this stream and compares the Outcome value with the ones defined in the **UnityVR.m** timing file. 

It contains information on: 
- Trial number
- Start position: X, Y, Z coordinates
- Fixation objects, materials and positions (both world and screen coordinates), size and fixation window
- Cue objects, materials and positions
- Target objects, materials and positions
- Distractor objects, materials and positions
- Outcome
- Unity computer LSL time

Again the **lslTracker.m** script adds 4 values: 

- LSL time correction between Unity (remote) and ML (local) clocks: remote clock = local clock + time correction
- Sample time: timestamp in Unity (remote) clock of sample being pushed to stream
- ML local LSL time: timestamp when sample is obtained
- ML trial time: time in milliseconds of ML trial (all other ML events use this time scale)
 
These values are saved to the `UserVars.VR_Trial` structure in MonkeyLogic _.bhv2_ files.

## Unity
To simplify the description we will separate the system in three sub-systems: the experiment design, the subject/monkey interface and the communication with MonkeyLogic. 

#### Experiment design
This section contains the only two classes that are openly modifiable: `ExperimentController` and `TaskInfo`. Both classes are `abstract` and need to be sub-classed in order to function. 

The `TaskInfo` class holds the values and references of all possible objects required to run a task. These variables include:
* array of possible start positions
* maximal trial duration
* the cue objects to use and their possible textures
* the possible targets, their color and positions
* the possible trial conditions, determining the cue/target/distractor texture associations

As this is an abstract class, it requires to be sub-classed and can thus be expanded to add new trial properties. Moreover, since all these properties are `public` they can be configured in the [[Property Inspector|2.3.2-Unity-Editor#property-inspector]] panel.

![TaskInfo](https://github.com/Doug1983/MTLab_UnitySource/blob/master/Documentation/img/Task_info.jpg)


The `ExperimentController` is a singleton that handles all the inner-workings of trial progression. It starts by getting all the data from TaskInfo to prepare a list of all trials and randomly samples from that list to set the values of the current trial. It further contains functions to execute at all the important time points. A non-exhaustive list is provided here but more details are available in the [source code](https://github.com/Doug1983/MTLab_UnitySource/blob/master/Assets/Scripts/Experiment/ExperimentController.cs#L401-L602). 

```C#
// List of available ExperimentController functions
protected virtual void Initialize() { /*...*/ }
public virtual void PrepareAllTrials() { /*...*/ }
public virtual void PrepareTrial() { /*...*/ }
public virtual void PrepareFixationObject() { /*...*/ }
public virtual void ShowFixationObject() { /*...*/ }
public virtual void HideFixationObject() { /*...*/ }
public virtual bool isFixating() { /*...*/ }
public virtual void PrepareCues()  { /*...*/ }
public virtual void ShowCues() { /*...*/ }
public virtual void HideCues() { /*...*/ }
public virtual void PrepareTargets() { /*...*/ }
public virtual void ShowTargets() { /*...*/ }
public virtual void HideTargets() { /*...*/ }
public virtual void PrepareDistractors() { /*...*/ }
public virtual void ShowDistractors() { /*...*/ }
public virtual void HideDistractors() { /*...*/ }
public virtual void FreezePlayer(bool ON) { /*...*/ }
public virtual void EndTrial() { /*...*/ }
```

The **first extremely important** thing to remember is that all these functions provide a default (i.e. virtual) behavior, but **if you want to add trial parameters to the TaskInfo class, you need to override these functions to take them into account**. For example, if I wanted to add a sound to a target object, I could add a `TargetSound` property to the sub-classed TaskInfo, but I would **also** need to `override` the ShowTargets() function to start playing the current trial sound, as sounds are not part of the default behaviors. More details in the [[Programming Tasks in Unity|3.3-Programming-tasks-in-unity]] section. 

The **second extremely important** thing to know is that the ExperimentController class has only the definitions of the functions to be called at the specific trial time points. We need another system to handle **when** these functions are called. Cue the [[StateSystem animator|2.3.2-Unity-Editor#animator]]. Each possible trial state has been already defined and their matching behavior scripts have been programmed. For example, when entering a state with a `TargetOnset` behavior attached to it, this script will get the ExperimenteController.instance reference and call the ShowTargets() function. This is a visual interface to program the trial state system. 

![Animator](https://github.com/Doug1983/MTLab_UnitySource/blob/master/Documentation/img/Animator.png)

Transitions between states (white arrows) are controlled via the `StateSystemCommon` behavior attached to each state. This script provides the conditions required to transition to the next state:
* Min/Max State Duration: random value in uniform distribution between Min and Max, if Max == 0 then the state lasts one frame, if Max == -1 state duration is infinite (i.e. elapsed time does not trigger a state change)
* Requires Fixation: checks whether gaze is within the fixation window of the fixation object defined in the TaskInfo class
* Trigger Type: 
    * None: No trigger volume collision required for state transition
    * Cue: waits for the subject to hit the collider(s) defined in the Cue Onset Triggers property of the TaskInfo
    * Target: same for Target Onset Triggers
    * Misc: Same for Misc Triggers

For example, the following image shows the parameters for the Pre-Cue delay: 

![SSC](https://github.com/Doug1983/MTLab_UnitySource/blob/master/Documentation/img/StateSystemCommon.jpg)

Since Max State Duration== -1, the system will stay in that state until the Trigger Type = Cue condition is met. In other words, the system will hold this state until either the trial time runs out or the subject collides with the trigger volume defined in the Cue Onset Triggers parameter of the TaskInfo. 

More details on how to design a new experiment in the [[next section|3.3-Programming-tasks-in-unity]].

#### Subject interface
Subjects can interface with the virtual environment either with an input device (keyboard, mouse, joystick or gamepad) or with their gaze via the EyeLink tracker. This system is fairly straightforward and should not require modification.

First, during the `Update` call, the `PlayerController` reads the currently selected input values and updates the subject's position within the environment, checking for collisions with trigger volumes or targets. On `LateUpdate` once all computations are done, it sends the current position, rotation, and collision states to the `ExperimentController`. 

Similarly, during `Update` the `EyeLinkController` reads the most recent eye sample from the EyeLink computer, converts the screen gaze position to 3D world positions and checks for the InstanceID of GameObjects falling under the gaze radius. To do so, it generates 33 "rays" (1 center point + 4 concentric circles) centered on gaze position in a 2DVA radius and returns the objects being hit with each ray. The list of 5 objects with most hits, as well as the number of rays hitting them, is sent to the `ExperimentController` on `LateUpdate`.

![GazeGrid](https://github.com/Doug1983/MTLab_UnitySource/blob/master/Documentation/img/gazeGrid.jpg)

#### MonkeyLogic interface
Once the `ExperimentController` has received all the data for the current frame (i.e. the Update and LateUpdate cycle) it waits for the absolute latest moment: [[WaitForEndOfFrame|2.3.3-Unity-Engine#yield-waitforendofframe]], before sending the data to the `MonkeyLogicController`, to be sent over the LSL streams. At the end of each trial, in the EndTrial() function of the ExperimentController, the current trial data is sent along the frame data. Trial data includes information about the currently selected parameters from TaskInfo, as well as the current outcome. 

During the `Update` cycle, the MonkeyLogicController simply checks for any control signals coming from MonkeyLogic, such as Pause or Resume the experiment. 

#### EventsController
The `EventsController` class is simply a convenience class to transfer events (i.e. function calls) across classes. Instead of relying on each class to have references to the instances of every other classes to trigger function calls, we create a singleton EventsController that relays the calls to the proper classes. 

For example, when sending PlayerController data to the ExperimentController. Instead of relying of instance references, we have the PlayerController call a function from the singleton instance of the EventsController, which then sends an Event for which the ExperimentController is listening for. This allows for MonkeyLogic control signals to simultaneously reach multiple classes, as multiple classes can listen in for a single event. 
